{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1527, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  label\n",
       "0  Huh, anyway check out this you[tube] channel: ...      1\n",
       "1  Hey guys check out my new channel and our firs...      1\n",
       "2             just for test I have to say murdev.com      1\n",
       "3    me shaking my sexy ass on my channel enjoy ^_^       1\n",
       "4             watch?v=vtaRGgvGtWQ   Check this out .      1"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score\n",
    "\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv('D:\\\\Praxis\\\\ML\\\\Data\\\\spam_new.csv')\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    778\n",
       "0    749\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of Ham and Spam comments\n",
    "spam.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comments    object\n",
       "label        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtypes\n",
    "spam.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(spam, test_size=0.2,random_state=1234,stratify=spam['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    156\n",
       "0    150\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"comments\"]= train[\"comments\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comments'] = train['comments'].str.replace('\\W', ' ') # Removes punctuation\n",
    "train['comments'] = train['comments'].str.lower()\n",
    "#X_train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train.comments.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train['comments'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting unique words in the dataset and storing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comments'] = train['comments'].str.split()\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "for sms in train['comments']:\n",
    "    for word in sms:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the number of times a word occured in a comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_comment = {word: [0] * len(train['comments']) for word in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, comments in enumerate(train['comments']):\n",
    "    for word in comments:\n",
    "        word_counts_per_comment[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.DataFrame(word_counts_per_comment)\n",
    "#word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1221, 3653)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_counts.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating the word count frame to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.concat([train.reset_index(drop=True), word_counts.reset_index(drop=True)], axis=1)\n",
    "#train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1221, 3655)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing probabilities and frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering spam and ham comments\n",
    "spam_comments = train_set[train_set['label'] == 1]\n",
    "ham_comments = train_set[train_set['label'] == 0]\n",
    "\n",
    "p_spam = len(spam_comments) / len(train_set) # probability of comment being spam\n",
    "p_ham = len(ham_comments) / len(train_set) # probability of comment being ham\n",
    "\n",
    "n_words_per_spam_comment = spam_comments['comments'].apply(len)\n",
    "n_spam = n_words_per_spam_comment.sum() # total_number_of_words_in_spam_comments\n",
    "\n",
    "n_words_per_ham_comment = ham_comments['comments'].apply(len)\n",
    "n_ham = n_words_per_ham_comment.sum() # total_number_of_words_in_spam_comments\n",
    "\n",
    "n_vocabulary = len(vocabulary) # number of unique words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 # laplace smoothing; useful to come out of 0 frequency problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALCULATING POSTERIOR PROBABILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "# P(word | spam) = (n(word|spam) + alpha) / (number of spam words + alpha * vocabulary)\n",
    "\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_comments[word].sum()\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "    n_word_given_ham = ham_comments[word].sum()\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING A CLASSIFIER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(comment,p_spam,p_ham,parameters_spam,parameters_ham):\n",
    "\n",
    "    label = 0\n",
    "    comment = re.sub('\\W', ' ', comment)\n",
    "    comment = comment.lower().split()\n",
    "\n",
    "    p_spam_given_comment = p_spam # initially it is set to the PRIOR probability of spam comment\n",
    "    p_ham_given_comment = p_ham\n",
    "    \n",
    "    for word in comment:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_comment *= parameters_spam[word]   # using property of conditional independence\n",
    "\n",
    "        if word in parameters_ham: \n",
    "            p_ham_given_comment *= parameters_ham[word]\n",
    "\n",
    "    #print('P(Spam|comment):', p_spam_given_comment)\n",
    "    #print('P(Ham|comment):', p_ham_given_comment)\n",
    "\n",
    "    if p_ham_given_comment > p_spam_given_comment:\n",
    "        label = 0\n",
    "    elif p_ham_given_comment < p_spam_given_comment:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 1 # We want to be cautious. Even if its a tie, we classify it as a spam comment\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_cold_CV(data):    \n",
    "    kf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None) \n",
    "    accuracy_list = []\n",
    "\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        \n",
    "        train, test = data.iloc[train_index], data.iloc[test_index]    \n",
    "    \n",
    "        train[\"comments\"]= train[\"comments\"].astype(str)\n",
    "        #train['label'] = y_train['label']\n",
    "        train['comments'] = train['comments'].str.replace('\\W', ' ') # Removes punctuation\n",
    "        train['comments'] = train['comments'].str.lower()\n",
    "\n",
    "        # unique words in the trainset\n",
    "\n",
    "        train['comments'] = train['comments'].str.split()\n",
    "\n",
    "        vocabulary = []\n",
    "\n",
    "        for sms in train['comments']:\n",
    "            for word in sms:\n",
    "                vocabulary.append(word)\n",
    "\n",
    "        vocabulary = list(set(vocabulary))\n",
    "\n",
    "        word_counts_per_comment = {word: [0] * len(train['comments']) for word in vocabulary}\n",
    "\n",
    "        for index, comments in enumerate(train['comments']):\n",
    "            for word in comments:\n",
    "                word_counts_per_comment[word][index] += 1\n",
    "\n",
    "        word_counts = pd.DataFrame(word_counts_per_comment)\n",
    "        word_counts.head()\n",
    "\n",
    "        train_set = pd.concat([train.reset_index(drop=True), word_counts.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        # Isolating spam and ham comments first\n",
    "        spam_comments = train_set[train_set['label'] == 1]\n",
    "        ham_comments = train_set[train_set['label'] == 0]\n",
    "\n",
    "        # P(Spam) and P(Ham)\n",
    "        p_spam = len(spam_comments) / len(train_set)\n",
    "        p_ham = len(ham_comments) / len(train_set)\n",
    "\n",
    "        # N_Spam\n",
    "        n_words_per_spam_comment = spam_comments['comments'].apply(len)\n",
    "        n_spam = n_words_per_spam_comment.sum()\n",
    "\n",
    "        # N_Ham\n",
    "        n_words_per_ham_comment = ham_comments['comments'].apply(len)\n",
    "        n_ham = n_words_per_ham_comment.sum()\n",
    "\n",
    "        # N_Vocabulary\n",
    "        n_vocabulary = len(vocabulary)\n",
    "\n",
    "        # Laplace smoothing\n",
    "        alpha = 1\n",
    "\n",
    "        # Initiate parameters\n",
    "        parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "        parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        # Calculate parameters\n",
    "        for word in vocabulary:\n",
    "            n_word_given_spam = spam_comments[word].sum() # spam_comments already defined\n",
    "            #print(\"{0}- {1},{2}\".format(word,n_word_given_spam,i))\n",
    "            i = i +1\n",
    "            p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "            parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "            n_word_given_ham = ham_comments[word].sum() # ham_comments already defined\n",
    "            p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "            parameters_ham[word] = p_word_given_ham\n",
    "                \n",
    "        test[\"comments\"]= test[\"comments\"].astype(str)\n",
    "        \n",
    "        test['comments'] = test['comments'].str.replace('\\W', ' ') # Removes punctuation\n",
    "        test['comments'] = test['comments'].str.lower()\n",
    "                \n",
    "        test['prediction'] = test['comments'].apply(classify, args=(p_spam,p_ham,parameters_spam,parameters_ham))\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(test['label'], test['prediction']).ravel()\n",
    "        \n",
    "        accuracy = (tp+tn)/(tn+ fp+ fn+ tp)\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "    return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9139344262295082"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_score = five_cold_CV(train)\n",
    "CV_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNNING THE MODEL ON THE TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['comments'] = test['comments'].str.replace('\\W', ' ') # Removes punctuation\n",
    "test['comments'] = test['comments'].str.lower()\n",
    "\n",
    "test['prediction'] = test['comments'].apply(classify,args=(p_spam,p_ham,parameters_spam,parameters_ham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[138,  12],\n",
       "       [  5, 151]], dtype=int64)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test['label'], test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(test['label'], test['prediction']).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SENSITIVITY, SPECIFICITY, PRECISION, ACCURACY all have values above 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.967948717948718"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity_tpr = tp/(fn+tp)\n",
    "sensitivity_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity_tnr = tn/(fp+tn)\n",
    "specificity_tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9263803680981595"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = tp/(tp + fp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn)/(tn+ fp+ fn+ tp)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model on WordPress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordpress_xmlrpc import Client\n",
    "from wordpress_xmlrpc.methods.posts import *\n",
    "from wordpress_xmlrpc.methods.users import *\n",
    "from wordpress_xmlrpc.methods.comments import *\n",
    "from wordpress_xmlrpc.methods.pages import *\n",
    " \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to retrieve comments in a post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_comments_per_post(client, post_id):\n",
    "\n",
    "    data = {'filter' : post_id,\n",
    "            'number': 2000,\n",
    "            'status': 'approve'}\n",
    " \n",
    "    resp = client.call(GetComments(data))\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code iterates over posts and then comments in each post, checks if they are spam and then deletes them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Comments...\n",
      "Checking in An unique case where Decision Tree outperforms the Random Forest...\n",
      "Checking in Finding Similar Players using Clustering Algorithm...\n",
      "Check complete...\n"
     ]
    }
   ],
   "source": [
    "from wordpress_xmlrpc.methods import posts\n",
    "\n",
    "wp = Client(r'https://mydataspace.wordpress.com/xmlrpc.php', 'username', 'password')\n",
    "print ('Searching for Comments...')\n",
    "\n",
    "posts_in_blog = wp.call(posts.GetPosts())\n",
    "\n",
    "for i in range(0, len(posts_in_blog)):\n",
    "    \n",
    "    print(\"Checking in {}...\".format(str(posts_in_blog[i])))\n",
    "    post_id = int(posts_in_blog[i].id)\n",
    "    \n",
    "    comments = get_all_comments_per_post(wp,post_id)\n",
    "    comments_string = []\n",
    "    \n",
    "    for j in range(0,len(comments)):\n",
    "        \n",
    "        comments_string.append(str(comments[j]))\n",
    "        if classify(comments_string[j],p_spam,p_ham,parameters_spam,parameters_ham):\n",
    "            print(\"deleting {}\".format(comments_string[j]))\n",
    "            wp.call(DeleteComment(comments[j].id))\n",
    "            \n",
    "print(\"Check complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
